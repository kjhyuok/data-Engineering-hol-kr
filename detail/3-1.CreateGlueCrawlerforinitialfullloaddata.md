# Lab: Data Validation and ETL

## PART A: Data Validation and ETL - Create Glue Crawler for initial full load data<br>
초기에 DMS를 통해 S3로 full load 되는 데이터를 위해 Glue Crawler를 생성해 봅니다.<br><br>
#### 1. 먼저 [**AWS Glue**](https://console.aws.amazon.com/glue/home) 로 이동합니다.
![3-2](https://user-images.githubusercontent.com/105655711/197322091-1206248a-1e51-464f-8699-feb3d58c57f1.png)

#### 2. AWS Glue menu에서 Crawlers를 선택하세요.
![3-3](https://user-images.githubusercontent.com/105655711/197322192-f7d1fffd-ae03-4542-a950-771ef1693e54.png)

#### 3. Add crawler 클릭해서 crawler 이름을 다음 ```glue-lab-crawler``` 로 입력해 주세요.

#### 4. 필요한 경우 description을 입력하시고 Next를 클릭합니다.
![3-4](https://user-images.githubusercontent.com/105655711/197322549-43c6cf83-439a-4518-b3aa-f4d9bbdf9214.png)

#### 5. **Data stores**, **Crawl all folders**을 선택하고 Next를 클릭합니다.
![3-5](https://user-images.githubusercontent.com/105655711/197322794-99e0b1fd-9804-476c-913b-a5619db3cef3.png)

#### 6. **Add a data store** 페이지에서 다음을 선택해주세요.
	a. Choose a data store에서 드롭다운 상자를 클릭하고 S3를 선택!
	b. Crawl data in에서 Specified path를 선택!
	c. Include path에는 앞서 CSV파일이 적재된 S3의 경로를 지정(ex> s3://xxx-dmslabs3bucket-xxx/tickets)

#### 7. 입력이 제대로 되었다면, 확인하시고 *Next*를 클릭해주세요.
![3-6](https://user-images.githubusercontent.com/105655711/197323218-92a189f4-fbd5-4468-9f72-a5f748377c79.png)

#### 8. Add another data store 에서 *No* 를 선택하고 *Next*를 클릭합니다.
![3-7](https://user-images.githubusercontent.com/105655711/197323390-b930a39c-3906-4fdf-b5f7-6db58693cde9.png)

#### 9. Choose an IAM role 페이지에서 다음을 선택해주세요.
	a. Choose an existing IAM role 선택!
	b. For IAM role, select <stackname>-GlueLabRole-<RandomString> pre-created for you. 예시는 “dmslab-student-GlueLabRole-ZOQDII7JTBUM”

#### 10. 입력이 제대로 되었다면, 확인하시고 *Next*를 클릭해주세요.
![3-8](https://user-images.githubusercontent.com/105655711/197323529-0a57634d-1762-49b2-87c5-785e6771b7a9.png)

#### 11. Create a schedule for this crawler 페이지에서, Frequency(빈도)에 Run on demand를 선택해주시고 *Next*를 클릭합니다.
![3-9](https://user-images.githubusercontent.com/105655711/197323658-99c39a1c-f441-4257-bfd0-c6ea91a10567.png) 

#### 12. Configure the crawler’s output 페이지에서, *Add database*를 클릭해서 Glue Catalogue 대한 새 데이터베이스를 생성해 봅니다. 
![3-10](https://user-images.githubusercontent.com/105655711/197323830-4218bed8-6cf5-4c0e-ab83-a7364fe99dee.png)

#### 13. Database Name은 ```ticketdata``` 를 입력하고 Create!<br/>
<img src="https://user-images.githubusercontent.com/105655711/197323929-4c4bf4a3-29a3-4539-bd04-fe9d7b964b98.png" width="50%" height="50%" title="px(픽셀) 크기 설정" alt="glue dbname"></img><br/>

#### 14. *Prefix added to tables (optional)* 는 빈칸으로 유지, *Configuration options (optional)*, 에서 *Add new columns only* 만 선택해 주시고 *Next*를 클릭합니다.
![3-12](https://user-images.githubusercontent.com/105655711/197324429-f8c8ecfc-6c58-43a4-835c-89ef061092b3.png) 

#### 15. 경로 및 데이터베이스 출력을 포함하는 Summary 페이지를 Review 하고 *Finish*를 클릭합니다. 이제 crawler를 실행할 준비가 되었습니다.
![3-13](https://user-images.githubusercontent.com/105655711/197324560-fe0549ac-3eac-4113-b6f8-07156ffcee79.png)

#### 16. crawler 이름 옆의 확인란을 선택하고 *Run crawler* 버튼을 클릭합니다. 
![3-14](https://user-images.githubusercontent.com/105655711/197324657-8ddd0537-d1ee-423c-bfaf-321e837e9fb4.png)
crawler 상태가 Start → Stop 으로 변경되고 crawler 다시 Ready 상태로 돌아올 때까지 기다리면(이 프로세스는 몇 분 정도 소요됨) 15개의 테이블이 생성되었음을 알 수 있습니다. 
![3-15](https://user-images.githubusercontent.com/105655711/197324661-77fb4c75-697a-4400-bec3-69535f97a918.png)

#### 17. AWS Glue 좌측의 탐색창에서, *Databases → Tables* 선택하고. 또한 *ticketdata* 데이터베이스를 클릭해서 테이블을 확인할 수 있습니다. 
![3-16](https://user-images.githubusercontent.com/105655711/197324724-b8ce8369-1469-412c-9089-cea04ac8fd5a.png)

여기까지 *Create Glue Crawler for initial full load data* 을 완료하셨습니다. 
이제 아래를 클릭하시어, 3-2. Data Validation Exercise를 위한 Lab으로 이동합니다.\
[3-2.Data Validation and ETL - Data Validation Exercise](../detail/3-2.DataValidationExercise.md)

